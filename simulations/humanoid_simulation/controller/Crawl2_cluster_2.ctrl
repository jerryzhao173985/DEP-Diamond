MATRIX 18 18
0.053839750000000006	0.0645866	-0.042201	-0.06986244999999999	-0.04970225	0.0473135	0.0656874	-0.061464500000000005	-0.07684515	0.054160150000000004	0.07459535	-0.05659715	0.0346011	-0.03454855	0.07532935	0.05811380000000001	-0.00637915	-0.029686450000000003
0.06609680000000001	0.13744420000000002	-0.06948670000000001	-0.11122135	-0.06420015000000001	0.061685199999999996	0.07878475000000001	-0.07632625000000001	-0.10687885000000001	0.0699736	0.11202955	-0.08377265	0.05747245	-0.02793725	0.10963865	0.09723685000000001	0.0160515	-0.049334100000000006
-0.04254775	-0.06976310000000001	0.05875085000000001	0.0688727	0.04358295	-0.04877335000000001	-0.05737135	0.07350034999999999	0.07257535	-0.054956050000000006	-0.07675635	0.05510790000000001	-0.03503055	0.02329445	-0.07244965	-0.06294115	-0.00237625	0.0202873
-0.06736575	-0.1142527	0.0695482	0.14361065	0.06203995	-0.06693305	-0.07689025000000001	0.08216445	0.10990250000000001	-0.08078669999999999	-0.11302114999999999	0.07697580000000001	-0.037293450000000006	0.0585079	-0.11238615000000002	-0.10159355	0.01159135	0.05193845
-0.0501306	-0.0634218	0.0434688	0.06143045	0.0641688	-0.0478475	-0.05466955	0.0574452	0.07081204999999999	-0.04546715	-0.07456575	0.0498435	-0.0363817	0.025824200000000002	-0.077205	-0.0492032	-0.0009130499999999992	0.0204468
0.048506850000000004	0.0657718	-0.04815475	-0.06802665000000001	-0.049058000000000004	0.062251	0.06361105	-0.0616744	-0.07088715	0.049514050000000004	0.07108715	-0.05540265	0.0338214	-0.028991300000000005	0.07856245	0.05716410000000001	0.00301595	-0.0210491
0.06360065000000001	0.073253	-0.054807400000000006	-0.07535275000000001	-0.05482745	0.06125435	0.10878625000000001	-0.07306225000000001	-0.09320485	0.06763269999999999	0.0906083	-0.06795265	0.029609299999999998	-0.027956750000000002	0.10114104999999998	0.06989575	0.0036642999999999997	-0.031112900000000002
-0.05577485	-0.06949374999999999	0.0699446	0.07551845000000001	0.05602925	-0.06126095	-0.06822025	0.1256296	0.09071895	-0.0734173	-0.0935239	0.07541445000000002	-0.014581150000000001	0.011213550000000001	-0.08798395	-0.0730281	0.012818199999999998	0.0329795
-0.0763392	-0.1090275	0.0725015	0.11178585000000002	0.07127945000000001	-0.07125700000000001	-0.0938843	0.09705805	0.13621935000000002	-0.0985182	-0.12272854999999999	0.08747645000000001	-0.044334500000000006	0.056770850000000005	-0.12122250000000001	-0.10232695	0.0029427	0.0588457
0.049304350000000004	0.0635802	-0.0530525	-0.06684575000000001	-0.04337395000000001	0.046924600000000004	0.06557355000000001	-0.0734497	-0.09116415	0.09964250000000001	0.0846546	-0.07891745	-0.0180168	-0.0168149	0.06878875000000001	0.06339875	0.0013574000000000001	-0.03912115
0.07476815	0.1129367	-0.07751675	-0.11256685	-0.07622065	0.07045435	0.09271430000000001	-0.10035705	-0.12630565	0.09456475	0.1454414	-0.10013545000000001	0.0635976	-0.0442225	0.1283329	0.1051151	0.00337215	-0.06376185000000001
-0.05114165	-0.07156415	0.05402415000000001	0.06799340000000001	0.04797285	-0.0513792	-0.06861175	0.075792	0.08354345	-0.07176935000000001	-0.09387145	0.10775400000000002	-0.01461895	-0.04522675	-0.07745715	-0.062228950000000005	0.00017850000000000006	0.04591365
0.054240800000000006	0.08273775	-0.04903565	-0.05678005	-0.0472209	0.045909900000000003	0.05397405	-0.0388072	-0.075703	-0.0031045	0.0896207	-0.04888885	0.27183064999999995	-0.17215995	0.09395195	0.06160020000000001	0.024088150000000003	-0.0017233
-0.05298335	-0.06029795	0.047620350000000006	0.0826164	0.03920900000000001	-0.04714985000000001	-0.04491445000000001	0.03250745	0.08503495	-0.040306850000000005	-0.07965485	-0.0122057	-0.1789363	0.28993744999999993	-0.08833275000000002	-0.07028709999999999	0.02327725	0.009456950000000002
0.07766970000000001	0.11009305000000001	-0.07192755	-0.11220395	-0.0788944	0.07866580000000001	0.10678025	-0.09178555	-0.12288569999999999	0.07466775	0.12777485	-0.0866	0.06914590000000001	-0.0603418	0.16696950000000002	0.10775055	-0.00481655	-0.042616
0.06016765	0.10056735	-0.0668462	-0.10563945	-0.04936550000000001	0.0568439	0.07428670000000001	-0.08108860000000001	-0.1014088	0.07554295	0.1048255	-0.07377955	0.0524279	-0.04919565	0.10895160000000001	0.1626786	0.0051254000000000004	-0.04218430000000001
-0.00058635	0.0139701	-0.00037894999999999997	0.010685150000000001	-0.008527449999999999	-0.0023251500000000002	0.00110655	0.0080159	0.0036589000000000005	-0.01073085	0.0027128000000000005	-0.0117616	0.0449288	0.039649500000000004	0.0024959	0.0008216499999999999	0.1268548	0.017937550000000003
-0.02480235	-0.043876399999999996	0.027861700000000003	0.05398955	0.016327750000000002	-0.02159655	-0.027110750000000003	0.0279651	0.055317000000000005	-0.044477600000000006	-0.05668985000000001	0.04194080000000001	0.01338245	0.0028393000000000003	-0.038846900000000004	-0.048645549999999996	0.028549750000000002	0.1425423
MATRIX 18 1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
MATRIX 18 18
1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0
0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0
0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0
0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0
0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0
0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0	0
0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0	0
0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0	0
0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0	0
0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0	0
0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0	0
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0	0
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1	0
0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	1
MATRIX 18 1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
[DEP,1.0][1]
 epsM=                   0.000000  learning rate of the model
 epsh=                   0.100000  learning rate of the controller bias
 synboost=               1.5 booster for synapses during motor signal creation
 timedist=               1.000000  time distance of product terms in learning rule
 urate=                  0.01000   update rate 
 evinterval=            10         interval to update eigenvalues etc (0: never) 
 indnorm=                0         individual normalization for each motor
 learningrule=           0         which learning rule to use: 0: DEP, 1: DHL, 2: HL, 
 regularization=        12         exponent of regularization 10^{-regularization}
 s4avg=                  1         smoothing (number of steps)
 s4delay=                1         delay  (number of steps)
######
